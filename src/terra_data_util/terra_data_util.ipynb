{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen3/Terra Data Table Utility Functions <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version:**  \n",
    "**Status:** This is Notebook is currently a **work in progress** and is not ready for general availability/use quite yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The primary purpose of this Notebook is combining multiple Gen3 graph-structured data tables in Terra to create a single consolidated table that is easier to use. The content of the combined table produced is configurable.\n",
    "\n",
    "The default behavior is to produce a table keyed by subject id, with one row per subject, for all subjects in a Terra Workspace. This table may include the genomic data, harmonized clinical metadata, or both, along with the associated administrative information.\n",
    "\n",
    "Additionally, convenience functions are provided for working with Terra data tables, including uploading, downloading, modification and deletion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements and Assumptions\n",
    "\n",
    "**Run in Terra**  \n",
    "This Notebook is intended to be used within the Terra Notebook environment using a Python 3 Jupyter kernel. \n",
    "\n",
    "**Workspace Data**   \n",
    "The consolidation is performed for all Gen3 data for the BioData Catalyst program in a Terra workspace. The data may be for subjects from one or more projects/cohorts.\n",
    "\n",
    "**Libraries**   \n",
    "The following libraries are expected to be available in the Terra Notebook environment, either by being preinstalled the 'Terra Notebook Runtime, Container Image, or explicit installation by the user:\n",
    "* `fiss` (version 0.16.23 or later)\n",
    "* `numpy` (version 1.15.2 or later)\n",
    "* `pandas` (0.25.3 or laster)\n",
    "* `tenacity` (6.1.0 or later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use\n",
    "\n",
    "The recommended way to use this Notebook is to \"import\" this Notebook into a user's primary Notebook using the `%run` command. The following steps added to a user's primary Notebook is sufficient to create a consolidated data table, in this example, a consolidated table containing both the genomic data and harmonized metadata:\n",
    "\n",
    "````\n",
    "! pip install tenacity\n",
    "\n",
    "%run terra_data_util.ipynb  \n",
    "\n",
    "BILLING_PROJECT_ID = os.environ['GOOGLE_PROJECT']  \n",
    "WORKSPACE = os.environ['WORKSPACE_NAME']\n",
    "\n",
    "consolidate_gen3_geno_pheno_tables(BILLING_PROJECT_ID, WORKSPACE, \"consolidated_metadata\")\n",
    "```\n",
    "\n",
    "This Notebook provides the following pre-defined functions for creating consolidated tables:\n",
    "* `consolidate_gen3_geno_pheno_tables(project: str, workspace: str, new_table_name: str) -> None`\n",
    "* `consolidate_gen3_geno_tables(project: str, workspace: str, new_table_name: str) -> None`\n",
    "* `consolidate_gen3_pheno_tables(project: str, workspace: str, new_table_name: str) -> None`\n",
    "\n",
    "These functions is available for processing custom merge specifications:\n",
    "* `consolidate_to_terra_table(project: str, workspace: str, merge_spec: dict, table_name: str) -> pd.DataFrame`\n",
    "* `consolidate_to_tsv(project: str, workspace: str, merge_spec: dict) -> str`\n",
    "* `consolidate_to_df(project: str, workspace: str, merge_spec: dict) -> pd.DataFrame`\n",
    "\n",
    "The Terra data tables that are included in the consolidated table, and how they are combined, is defined by a merge specification defined as a Python dictionary.\n",
    "The merge specification supports standard SQL-style join operations and can be customized as desired.\n",
    "\n",
    "The following functions explicitly merge individual tables.  \n",
    "These may be useful, for example, for merging user-provided data with a consolidated table produced by the functions above.\n",
    "* `merge_terra_tables_to_table(project: str, workspace: str, left_table_name: str, left_table_previously_consolidated: bool, right_table_name: str, right_table_previously_consolidated: bool, join_type: str, join_column: str, final_index_source_column: str, result_table_name: str, **kwargs) -> None`\n",
    "* `merge_terra_tables_to_df(project: str, workspace: str, left_table_name: str, left_table_previously_consolidated: bool, right_table_name: str, right_table_previously_consolidated: bool, join_type: str, join_column: str, **kwargs) -> pd.DataFrame`\n",
    "* `merge_df_and_terra_table_to_df(project: str, workspace: str, left_df: pd.DataFrame, right_table_name: str, right_table_previously_consolidated: bool, join_type: str, join_column: str, **kwargs) -> pd.DataFrame`\n",
    "\n",
    "These convenience functions are also available:\n",
    "* `set_entity_attribute_value(project: str, workspace: str, entity_type: str, entity_name: str, attribute_name: str, value: Union[list, bool, int, float, str, None]) -> None:`\n",
    "\n",
    "* `delete_terra_table(project: str, workspace: str, table_name: str) -> None`\n",
    "* `delete_all_gen3_tables(project: str, workspace: str) -> None`\n",
    "* `get_terra_table_to_df(project: str, workspace: str, table_name: str, attributeNames=None, model=\"flexible\") -> pd.DataFrame:`\n",
    "* `rename_df_column(df: pd.DataFrame, current_column_name: str, new_column_name: str) -> None`\n",
    "* `write_df_to_tsv_file(df: pd.DataFrame, filename: str) -> None:`\n",
    "* `upload_entities_df(project: str, workspace: str, df: pd.DataFrame, chunk_size=500) -> None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it Works\n",
    "\n",
    "This Notebook uses the Broad FireCloud API to read each Terra data table identified in the merge specification into a Pandas DataFrame and performs SQL-style joins on the tables using the Pandas `merge` operation to produce a single, consolidated table.\n",
    "References in the Gen3 data model are only the direction of the graph leaf/bottom nodes to the root/top node.\n",
    "\n",
    "During this consolidation process, the name of each column in a table is prefixed with the name of the table it is from. Additionally, the columns containing entity ids have the `_entity_id` suffix appended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that a recent version of firecloud is installed.\n",
    "The version must be 0.16.23 or later for flexible entity support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade firecloud\n",
    "# ! pip show firecloud\n",
    "# ! pip install tenacity\n",
    "# ! pip install pysnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import resource\n",
    "import sys\n",
    "from typing import Union\n",
    "\n",
    "from firecloud import fiss\n",
    "from firecloud.errors import FireCloudServerError\n",
    "import firecloud.api as fapi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tenacity\n",
    "import time\n",
    "from tenacity import retry, after_log, before_sleep_log, retry_if_exception_type, stop_after_attempt, wait_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import INFO, DEBUG\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonly Used Merge Specifications and Convenience Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create a consolidated data table containing both genomic and phenotypic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GEN3_GENO_PHENO_MERGE_SPEC = {\n",
    "  \"default_join_type\": \"outer\",\n",
    "  \"merge_sequence\": [\n",
    "    {\n",
    "      \"join_column\": \"simple_germline_variation\",\n",
    "      \"table_names\": [\"simple_germline_variation\", \"germline_variation_index\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"submitted_aligned_reads\",\n",
    "      \"table_names\": [\"submitted_aligned_reads\", \"aligned_reads_index\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"read_group\",\n",
    "      \"table_names\": [\"read_group\", \"submitted_unaligned_reads\", \"read_group_qc\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"aliquot\",\n",
    "      \"table_names\": [\"aliquot\", \"submitted_cnv_array\", \"submitted_snp_array\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"sample\",\n",
    "      \"table_names\": [\"sample\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"subject\",\n",
    "      \"table_names\": [\"subject\", \"blood_pressure_test\", \"cardiac_mri\", \"demographic\", \"electrocardiogram_test\", \"exposure\", \"lab_result\", \"medical_history\", \"medication\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"study\",\n",
    "      \"table_names\": [\"study\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"project\",\n",
    "      \"table_names\": [\"project\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"program\",\n",
    "      \"table_names\": [\"program\"]\n",
    "    }\n",
    "  ],\n",
    "  \"final_index_source_column\": \"subject_submitter_id\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def consolidate_gen3_geno_pheno_tables(project: str, workspace: str, new_table_name: str, **kwargs) -> None:\n",
    "    consolidate_to_terra_table(project, workspace, GEN3_GENO_PHENO_MERGE_SPEC, new_table_name, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create a consolidated data table containing only genomic (no phenotypic) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GEN3_GENO_MERGE_SPEC =  {\n",
    "  \"default_join_type\": \"outer\",\n",
    "  \"merge_sequence\": [\n",
    "    {\n",
    "      \"join_column\": \"simple_germline_variation\",\n",
    "      \"table_names\": [\"simple_germline_variation\", \"germline_variation_index\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"submitted_aligned_reads\",\n",
    "      \"table_names\": [\"submitted_aligned_reads\", \"aligned_reads_index\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"read_group\",\n",
    "      \"table_names\": [\"read_group\", \"submitted_unaligned_reads\", \"read_group_qc\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"aliquot\",\n",
    "      \"table_names\": [\"aliquot\", \"submitted_cnv_array\", \"submitted_snp_array\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"sample\",\n",
    "      \"table_names\": [\"sample\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"subject\",\n",
    "      \"table_names\": [\"subject\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"study\",\n",
    "      \"table_names\": [\"study\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"project\",\n",
    "      \"table_names\": [\"project\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"program\",\n",
    "      \"table_names\": [\"program\"]\n",
    "    }\n",
    "  ],\n",
    "  \"final_index_source_column\": \"subject_submitter_id\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def consolidate_gen3_geno_tables(project: str, workspace: str, new_table_name: str, **kwargs) -> None:\n",
    "    consolidate_to_terra_table(project, workspace, GEN3_GENO_MERGE_SPEC, new_table_name, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create a consolidated data table containing only phenotypic (no genomic) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note: Here the \"sample\" table is being included in the phenotypic data because it contains useful identifier information (e.g., the \"NWD\" identifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GEN3_PHENO_MERGE_SPEC =  {\n",
    "  \"default_join_type\": \"outer\",\n",
    "  \"merge_sequence\": [\n",
    "    {\n",
    "      \"join_column\": \"subject\",\n",
    "      \"table_names\": [\"subject\", \"sample\", \"blood_pressure_test\", \"cardiac_mri\", \"demographic\", \"electrocardiogram_test\", \"exposure\", \"lab_result\", \"medical_history\", \"medication\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"study\",\n",
    "      \"table_names\": [\"study\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"project\",\n",
    "      \"table_names\": [\"project\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"program\",\n",
    "      \"table_names\": [\"program\"]\n",
    "    }\n",
    "  ],\n",
    "  \"final_index_source_column\": \"subject_submitter_id\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def consolidate_gen3_pheno_tables(project: str, workspace: str, new_table_name: str, **kwargs) -> None:\n",
    "    consolidate_to_terra_table(project, workspace, GEN3_PHENO_MERGE_SPEC, new_table_name, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Custom Merge Specification and Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GEN3_USER_CUSTOM_MERGE_SPEC =  {\n",
    "  \"default_join_type\": \"inner\",\n",
    "  \"merge_sequence\": [\n",
    "    {\n",
    "      \"join_column\": \"simple_germline_variation\",\n",
    "      \"table_names\": [\"simple_germline_variation\", \"germline_variation_index\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"submitted_aligned_reads\",\n",
    "      \"table_names\": [\"submitted_aligned_reads\", \"aligned_reads_index\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"read_group\",\n",
    "      \"table_names\": [\"read_group\", \"submitted_unaligned_reads\", \"read_group_qc\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"aliquot\",\n",
    "      \"table_names\": [\"aliquot\", \"submitted_cnv_array\", \"submitted_snp_array\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"sample\",\n",
    "      \"table_names\": [\"sample\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"subject\",\n",
    "      \"join_type\": \"left\",\n",
    "      \"table_names\": [\"subject\", \"blood_pressure_test\", \"cardiac_mri\", \"demographic\", \"electrocardiogram_test\", \"exposure\", \"lab_result\", \"medical_history\", \"medication\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"study\",\n",
    "      \"table_names\": [\"study\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"project\",\n",
    "      \"table_names\": [\"project\"]\n",
    "    },\n",
    "    {\n",
    "      \"join_column\": \"program\",\n",
    "      \"table_names\": [\"program\"]\n",
    "    }\n",
    "  ],\n",
    "  \"final_index_source_column\": \"subject_submitter_id\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def consolidate_gen3_custom_tables(project: str, workspace: str, new_table_name: str, **kwargs) -> None:\n",
    "    consolidate_to_terra_table(project, workspace, GEN3_USER_CUSTOM_MERGE_SPEC, new_table_name, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Convenience Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the merges defined in the merge specification and upload the resulting table to Terra the given name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def consolidate_to_terra_table(project: str, workspace: str, merge_spec: dict, table_name: str, **kwargs) -> None:\n",
    "    \n",
    "    # dest_project and dest_workspace can be used to write the consolidated data table\n",
    "    # to a different billing project and workspace than the one contining the Gen3 data tables.\n",
    "    # This can be useful for working around some of the current Terra scale limitations, \n",
    "    # and for other researcher use cases.\n",
    "    \n",
    "    dest_project = kwargs.get(\"dest_project\", project)\n",
    "    dest_workspace = kwargs.get(\"dest_workspace\", workspace)\n",
    "    \n",
    "    if 'final_index_source_column' in merge_spec and len(merge_spec['final_index_source_column']):\n",
    "        entity_id_column = merge_spec['final_index_source_column']\n",
    "    else:\n",
    "        logger.error(\"The merge specification field \\\"final_index_source_column\\\" is missing or has an empty value.\")\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Starting data consolidation to table \\\"{}\\\".\".format(table_name)); _flush_log();\n",
    "    \n",
    "    _check_and_log_existing_table(dest_project, dest_workspace, table_name)\n",
    "    \n",
    "    consolidated_df = consolidate_to_df(project, workspace, merge_spec)\n",
    " \n",
    "    # Add \"entity:{table_name}_id\" as the first column, as required by Terra.\n",
    "    consolidated_df.insert(0, f\"entity:{table_name}_id\", consolidated_df[entity_id_column])\n",
    "    \n",
    "    upload_entities_df_and_verify(dest_project, dest_workspace, consolidated_df) \n",
    "        \n",
    "    logger.info(_get_python_resource_usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the merges defined in the merge specification and return the resulting table in a TSV format string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def consolidate_to_tsv(project: str, workspace: str, merge_spec: dict)  -> str:\n",
    "    return consolidate_to_df(project, workspace, merge_spec).to_csv(sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the merges defined in the merge specification and return the resulting table as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# @pysnooper.snoop\n",
    "def consolidate_to_df(project: str, workspace: str, merge_spec: dict)  -> pd.DataFrame:\n",
    "    default_merge_parameters = merge_spec['default_merge_parameters'] if 'default_merge_parameters' in merge_spec else dict(how=\"outer\")\n",
    "    if \"default_join_type\" in merge_spec:\n",
    "        default_merge_parameters['how'] = merge_spec['default_join_type']\n",
    "        \n",
    "    merged_df = None\n",
    "    for merge_info in merge_spec['merge_sequence']:\n",
    "        merge_parameters = _create_combined_merge_parameters(default_merge_parameters, merge_info)\n",
    "        _substitute_entity_id_column_name(merge_parameters)\n",
    "        merged_df = _consolidate_tables_to_df(project, workspace, merge_info['table_names'], merge_parameters, merged_df)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge two Terra tables to a Terra table using the specified merge parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def merge_terra_tables_to_table(project: str, workspace: str,\n",
    "                                left_table_name: str, left_table_previously_consolidated: bool,\n",
    "                                right_table_name: str, right_table_previously_consolidated: bool,\n",
    "                                join_type: str, join_column: str,\n",
    "                                final_index_source_column: str, result_table_name: str,\n",
    "                                **kwargs) -> None:\n",
    "    \n",
    "    dest_project = kwargs.get(\"dest_project\", project)\n",
    "    dest_workspace = kwargs.get(\"dest_workspace\", workspace)\n",
    "    \n",
    "    _check_and_log_existing_table(dest_project, dest_workspace, result_table_name)\n",
    "    \n",
    "    merged_df = merge_terra_tables_to_df(project, workspace,\n",
    "                                left_table_name, left_table_previously_consolidated,\n",
    "                                right_table_name, right_table_previously_consolidated,\n",
    "                                join_type, join_column, **kwargs)\n",
    "    \n",
    "    # Add \"entity:{entity_name}_id\" as the first column, as required by Terra.\n",
    "    merged_df.insert(0, f\"entity:{result_table_name}_id\", merged_df[final_index_source_column])\n",
    "    \n",
    "    upload_entities_df_and_verify(dest_project, dest_workspace, merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge two Terra tables to a Pandas DataFrame using the specified merge parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def merge_terra_tables_to_df(project: str, workspace: str,\n",
    "                            left_table_name: str, left_table_previously_consolidated: bool,\n",
    "                            right_table_name: str, right_table_previously_consolidated: bool,\n",
    "                            join_type: str, join_column: str, **kwargs) -> pd.DataFrame: \n",
    "    if left_table_previously_consolidated:\n",
    "        left_table_df = get_terra_table_to_df(project, workspace, left_table_name)\n",
    "    else:\n",
    "        left_df = get_gen3style_terra_table_to_df(project, workspace, left_table_name)\n",
    "        \n",
    "    merged_df = merge_df_and_terra_table_to_df(project, workspace,\n",
    "                                               left_df,\n",
    "                                               right_table_name, right_table_previously_consolidated,\n",
    "                                               join_type, join_column, **kwargs)\n",
    "        \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge a DataFrame and a Terra table to a Panda DataFrame using the specified merge parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def merge_df_and_terra_table_to_df(project: str, workspace: str,\n",
    "                            left_df: pd.DataFrame,\n",
    "                            right_table_name: str, right_table_previously_consolidated: bool,\n",
    "                            join_type: str, join_column: str, **kwargs) -> pd.DataFrame:\n",
    "        \n",
    "    if right_table_previously_consolidated:\n",
    "        right_table_df = get_terra_table_to_df(project, workspace, right_table_name)\n",
    "    else:\n",
    "        right_df = get_gen3style_terra_table_to_df(project, workspace, right_table_name)\n",
    "        \n",
    "    merge_parameters = dict(how=join_type, on=join_column, **kwargs)\n",
    "    merged_df = left_df.merge(right_df, **merge_parameters)\n",
    "    \n",
    "    logger.info(\"Merged table \\\"{}\\\" on column \\\"{}\\\" with join type: \\\"{}\\\". New merged table dimmensions: ({}x{})\".format(\n",
    "    right_table_name, merge_parameters['on'], merge_parameters['how'], merged_df.shape[0], merged_df.shape[1]))\n",
    "    \n",
    "    if logger.isEnabledFor(DEBUG):\n",
    "        logger.debug(\"The in-memory merged data frame size is: {} rows x {} columns\".format(merged_df.shape[0], merged_df.shape[1]))\n",
    "        _debug_write_df_to_tsv_file(merged_df, \"merged_df\")\n",
    "        \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pandas DataFrame containing the contents of the given Terra data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_terra_table_to_df(project: str, workspace: str, table_name: str, attributeNames=None, model=\"flexible\") -> pd.DataFrame:\n",
    "    data_table_info = DataTableInfo(project, workspace)\n",
    "    row_count, _, _ = data_table_info.get_table_info(table_name)\n",
    "    single_read_max_size = 5000\n",
    "    if row_count <= single_read_max_size:\n",
    "        # Process as a single read operation\n",
    "        response = _fapi_get_entities_tsv(project, workspace, table_name, attributeNames, model=model)\n",
    "        table_df = pd.read_csv(io.StringIO(response.text), sep='\\t')\n",
    "    else:\n",
    "        table_df = _get_large_terra_table_to_df(project, workspace, table_name, attributeNames)\n",
    "    \n",
    "    # Change the dataframe index from the default numeric index to the the entity id column.\n",
    "    # TODO - Resetting the index below had the unexpected effect of causing the subsequent merge\n",
    "    #        operation to fail due to a key error, even though the intended key was present\n",
    "    #        in both tables. Omit the following until it can be investigated and resolved.\n",
    "    # table_df.set_index(f\"entity:{table_name}_id\", inplace=True)\n",
    "    \n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pandas DataFrame containing the contents of the given Terra data table, with columns renamed to facilitate merging:\n",
    "* In general, column names are prefixed with the name of the table to address conflicts that would otherwise occur due to fields having the same name in multiple different tables.  \n",
    "* Columns representiong relationships between tables are suffixed with `_entity_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_gen3style_terra_table_to_df(project: str, workspace: str, table_name: str, model=\"flexible\") -> pd.DataFrame:\n",
    "    table_df = get_terra_table_to_df(project, workspace, table_name)\n",
    "    columns = table_df.columns\n",
    "    rename_df_column(table_df, f\"entity:{table_name}_id\", f\"{table_name}_entity_id\") # Column 0\n",
    "    for column in columns[1:]:\n",
    "        if column in _GEN3_TABLE_NAMES:\n",
    "            rename_df_column(table_df, column, f\"{column}_entity_id\")\n",
    "        else:\n",
    "            rename_df_column(table_df, column, f\"{table_name}_{column}\")\n",
    "    # Deduplicate \"*_entity_id\" columns\n",
    "    table_df = table_df.loc[:,~table_df.columns.duplicated()]\n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename a column in the given Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def rename_df_column(df: pd.DataFrame, current_column_name: str, new_column_name: str) -> None:\n",
    "    df.rename(columns={current_column_name : new_column_name}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the contents of the Pandas DataFrame to given filename on the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def write_df_to_tsv_file(df: pd.DataFrame, filename: str) -> None:\n",
    "    with open(filename, mode=\"w\") as tsv_file:\n",
    "        tsv_string = df.to_csv(sep=\"\\t\", index=False)\n",
    "        tsv_file.write(tsv_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the contents of the Pandas DataFrame to a Terra data table.  \n",
    "This includes support for \"chunking\" large tables into smaller sections that can be successfully uploaded individually.\n",
    "\n",
    "Note: The format of the table within the Pandas DataFrame must comform to the format described here: https://support.terra.bio/hc/en-us/articles/360025758392-Managing-data-with-tables-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def upload_entities_df(project: str, workspace: str, df: pd.DataFrame, chunk_size=500) -> None:\n",
    "    logger.info(\"Starting upload of data table to Terra.\"); _flush_log()\n",
    "    chunk_start = chunk_end = 0\n",
    "    row_count = df.shape[0]\n",
    "    first_iteration = True\n",
    "    start_time = time.time()\n",
    "    while chunk_start < row_count:\n",
    "        chunk_end = min(chunk_start + chunk_size, row_count)\n",
    "        chunk_df = df.iloc[chunk_start:chunk_end]\n",
    "        chunk_tsv = chunk_df.to_csv(sep=\"\\t\", index=False)\n",
    "        _fapi_upload_entities(project, workspace, chunk_tsv, \"flexible\")\n",
    "        chunk_start = chunk_end\n",
    "        if first_iteration:\n",
    "            first_iteration = False\n",
    "            iteration_duration = time.time() - start_time\n",
    "            estimated_duration = _estimate_total_duration(row_count, chunk_size, iteration_duration)\n",
    "            logger.info(\"Estimated time to upload table with {} rows: {}\".format(row_count, estimated_duration))\n",
    "            _output_now(\"Uploading \")\n",
    "        _output_now(\".\")\n",
    "    _output_now(\"\\n\")\n",
    "    total_duration = time.time() - start_time\n",
    "    logger.info(\"\\nFinished uploading data table in {} minutes.\".format(str(round((total_duration / 60), 1))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def upload_entities_df_and_verify(project: str, workspace: str, df: pd.DataFrame, chunk_size=500) -> None:\n",
    "    \n",
    "    entity_id_column_name = df.columns[0]\n",
    "    if not re.match(\"entity:.+_id\", entity_id_column_name):\n",
    "        raise TerraDataUtilException(\"The first column name does not match the pattern required to upload: {}\".formt(entity_id_column_name))\n",
    "    table_name = entity_id_column_name.replace(\"entity:\", \"\").replace(\"_id\", \"\")  \n",
    "    \n",
    "    df_rows, df_columns = df.shape\n",
    "    if logger.isEnabledFor(DEBUG):\n",
    "        logger.info(\"The in-memory data table size is: {} rows x {} columns\".format(df.shape[0], df.shape[1]))\n",
    "        _debug_write_df_to_tsv_file(df, \"upload_df\")\n",
    "    \n",
    "    upload_entities_df(project, workspace, df, chunk_size)\n",
    "    \n",
    "    # Compare the in-memory and actual uploaded data table sizes and output the results.\n",
    "    data_table_info = DataTableInfo(project, workspace)\n",
    "    actual_rows, actual_columns, _ = data_table_info.get_table_info(table_name)\n",
    "    if (df_rows == actual_rows and df_columns == actual_columns):\n",
    "        logger.info(\"The data table \\\"{}\\\" size is: {} rows x {} columns\".format(\n",
    "            table_name, actual_rows, actual_columns))\n",
    "    else:\n",
    "        if (df_rows > actual_rows or df_columns > actual_columns):\n",
    "            logger.error(\"Data table truncation error.\"\n",
    "                         \" The in-memory data table has more rows or columns ({}x{}) than the data table \\\"{}\\\" uploaded to Terra ({}x{})\".format(\n",
    "                           df_rows, df_columns, table_name, actual_rows, actual_columns))\n",
    "        else:\n",
    "            logger.warning(\"Data table size mismatch warning.\"\n",
    "                           \" The in-memory data table has fewer rows or columns ({}x{}) than the data table \\\"{}\\\" uploaded to Terra ({}x{})\".format(\n",
    "                           df_rows, df_columns, table_name, actual_rows, actual_columns)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add or update the given attribute and value into a Terra data table or set.  \n",
    "This may be used, for example, to set an array of strings into a cell, for subsequent use as a workflow input parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def set_entity_attribute_value(project: str, workspace: str, entity_type: str, entity_name: str, attribute_name: str,\n",
    "                               value: Union[list, bool, int, float, str, None]) -> None:\n",
    "    # See: https://software.broadinstitute.org/firecloud/documentation/article?id=10892\n",
    "    set_attribute_json = [fapi._attr_set(attribute_name, value)]\n",
    "    _fapi_update_entity(project, workspace, entity_type, entity_name, set_attribute_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the Terra data table with the given billing project, workspace and name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def delete_terra_table(project: str, workspace: str, table_name: str) -> None:\n",
    "    logger.info(\"Preparing to delete data table \\\"{}\\\" ...\".format(table_name)); _flush_log()\n",
    "    if table_name not in DataTableInfo(project, workspace).get_table_names():\n",
    "        logger.warning(\"Data table \\\"{}\\\" not found.\".format(table_name))\n",
    "        return\n",
    "    \n",
    "    logger.info(\"Starting deletion of data table \\\"{}\\\"\".format(table_name)); _flush_log()\n",
    "    entity_id_series = get_table_entity_ids_to_series(project, workspace, table_name)\n",
    "    chunk_size = 100\n",
    "    num_chunks = math.ceil(entity_id_series.size / chunk_size)\n",
    "    first_iteration = True\n",
    "    start_time = time.time()\n",
    "    for chunk in  np.array_split(entity_id_series, num_chunks):\n",
    "        # The FireCloud API requires entity ids to be strings, not a numeric type.\n",
    "        # chunk_as_strings = [str(id) for id in chunk]\n",
    "        response = _fapi_delete_entity_type(project, workspace, table_name, chunk)\n",
    "        if first_iteration:\n",
    "            first_iteration = False\n",
    "            iteration_duration = time.time() - start_time\n",
    "            estimated_duration = _estimate_total_duration(entity_id_series.size, chunk_size, iteration_duration)\n",
    "            logger.info(\"Estimated time to delete table with {} rows: {}\".format(entity_id_series.size, estimated_duration))\n",
    "            _output_now(\"Deleting \")\n",
    "        _output_now(\".\")\n",
    "    _output_now(\"\\n\")\n",
    "    total_duration = time.time() - start_time\n",
    "    logger.info(\"\\nFinished deleting data table \\\"{}\\\" in {} minutes.\".format(table_name, str(round((total_duration / 60), 1))))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all Gen3 data tables in the given billing project and workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def delete_all_gen3_tables(project: str, workspace: str):\n",
    "    logger.info(\"Deleting all Gen3 tables in workspace \\\"{}\\\". This may require a very long time depending on the number and size of the Gen3 tables.\".format(workspace))\n",
    "    data_table_info = DataTableInfo(project, workspace)\n",
    "    for gen3_table_name in _GEN3_TABLE_NAMES:\n",
    "        if gen3_table_name in data_table_info.get_table_names():\n",
    "            delete_terra_table(project, workspace, gen3_table_name)\n",
    "    logger.info(\"Finished deleting all Gen3 tables in workspace \\\"{}\\\".\".format(workspace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the full list of entity ids for a Terra data table as a Series of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_table_entity_ids_to_series(project: str, workspace: str, table_name: str) -> pd.Series:\n",
    "    entity_id_column_name = f\"entity:{table_name}_id\"\n",
    "    response = _fapi_get_entities_tsv(project, workspace, table_name, attributeNames=[entity_id_column_name], model=\"flexible\")\n",
    "    table_df = pd.read_csv(io.StringIO(response.text), sep='\\t', usecols=[entity_id_column_name], dtype=\"str\")\n",
    "    entity_id_series = table_df[entity_id_column_name]\n",
    "    return entity_id_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data and functions used internally and not intended for user modification.  \n",
    "*The code in the rest of this document will likely be moved to a new Python library \"soon\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the list of tables defined in the Gen3 data model, for use internal to this Notebook.  \n",
    "All of the tables used in merge specications must exist in this list, yet this list may contain additional tables names are not used in the merge specifications and do not exist in the current workspace.\n",
    "\n",
    "For use when deleting all Gen3 tables, this list must be a partial ordering based on the Gen3 dependencies between tables, from the leaves of the Gen3 graph data model to the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "_GEN3_TABLE_NAMES = [\n",
    "    \"germline_variation_index\",\n",
    "    \"simple_germline_variation\",\n",
    "    \"aligned_reads_index\",\n",
    "    \"submitted_unaligned_reads\",\n",
    "    \"submitted_aligned_reads\",\n",
    "    \"read_group_qc\",\n",
    "    \"read_group\",\n",
    "    \"submitted_cnv_array\",\n",
    "    \"submitted_snp_array\", \"aliquot\",\n",
    "    \"sample\",\n",
    "    \"blood_pressure_test\",\n",
    "    \"cardiac_mri\",\n",
    "    \"demographic\",\n",
    "    \"electrocardiogram_test\",\n",
    "    \"exposure\",\n",
    "    \"lab_result\",\n",
    "    \"medical_history\",\n",
    "    \"medication\",\n",
    "    \"subject\",\n",
    "    \"study\",\n",
    "    \"reference_file_index\",\n",
    "    \"reference_file\",\n",
    "    \"project\",\n",
    "    \"program\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if logger.isEnabledFor(DEBUG):\n",
    "    %xmode Verbose\n",
    "    import pysnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# @pysnooper.snoop()\n",
    "def _consolidate_tables_to_df(project: str, workspace: str, table_names: list, merge_parameters: dict, initial_df = None) -> pd.DataFrame:\n",
    "    if initial_df is None:\n",
    "        assert len(table_names) >= 2, \"At least two table names are required.\" \n",
    "        table_name = table_names[0]\n",
    "        table_names = table_names[1:]\n",
    "        first_df = get_gen3style_terra_table_to_df(project, workspace, table_name)\n",
    "        if table_name == \"sample\":\n",
    "            _deduplicate_merge_data(None, first_df, \"sample\", _get_entity_id_column_name(\"subject\"))\n",
    "        merged_df = first_df\n",
    "    else:\n",
    "        assert len(table_names) >= 1, \"At least one table names is required to merge with previous data.\"\n",
    "        merged_df = initial_df\n",
    "        \n",
    "    data_table_info = DataTableInfo(project, workspace)\n",
    "    for table_name in table_names:\n",
    "        if table_name not in data_table_info.get_table_names():\n",
    "            logger.info(\"The table \\\"{}\\\" was not found in this workspace and will be ignored.\".format(table_name))\n",
    "            continue            \n",
    "        current_df = get_gen3style_terra_table_to_df(project, workspace, table_name)\n",
    "        if table_name == \"sample\":\n",
    "            _deduplicate_merge_data(merged_df, current_df, \"sample\", _get_entity_id_column_name(\"subject\"))\n",
    "            \n",
    "        if logger.isEnabledFor(DEBUG):\n",
    "            _debug_write_df_to_tsv_file(merged_df, \"merged_df\")\n",
    "            _debug_write_df_to_tsv_file(current_df, \"current_df\")\n",
    "            \n",
    "        logger.debug(\"Merging table \\\"{}\\\" using column \\\"{}\\\" with join type: {}\".format(\n",
    "            table_name, merge_parameters['on'], merge_parameters['how']))\n",
    "        logger.debug(\"Full merge parameters: {}\".format(merge_parameters))\n",
    "        \n",
    "        merged_df = _merge_dataframes(merged_df, current_df, **merge_parameters)\n",
    "        \n",
    "        logger.info(\"Merged table \\\"{}\\\" on column \\\"{}\\\" with join type: \\\"{}\\\". New merged table dimmensions: ({}x{})\".format(\n",
    "            table_name, merge_parameters['on'], merge_parameters['how'], merged_df.shape[0], merged_df.shape[1]))\n",
    "        \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _get_entity_id_column_name(entity_type: str):\n",
    "    return f\"{entity_type}_entity_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _check_and_log_existing_table(project: str, workspace:str , table_name:str) -> None:\n",
    "    data_table_info = DataTableInfo(project, workspace)\n",
    "    if (table_name in data_table_info.get_table_names()):\n",
    "        existing_rows, existing_columns, _ = data_table_info.get_table_info(table_name)\n",
    "        logger.info(\"A data table with the name \\\"{}\\\" already exists with dimmesions ({}x{}). Corresponding data will be updated and any existing additional data will be left unchanged.\".format(\n",
    "        table_name, existing_rows, existing_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     37
    ]
   },
   "outputs": [],
   "source": [
    "def _get_large_terra_table_to_df(project: str, workspace: str, table_name: str, attributeNames=None) -> pd.DataFrame:\n",
    "    total_row_count, _, _ = DataTableInfo(project, workspace).get_table_info(table_name)\n",
    "    page_size = 5000\n",
    "    num_pages = int(math.ceil(float(total_row_count) / page_size))\n",
    "    entity_results_list = []\n",
    "    for i in range(1, num_pages + 1):\n",
    "        entity_results_list.append(_fapi_get_entities_query(project, workspace, table_name, i, page_size).json())\n",
    "\n",
    "    row_jsons = []\n",
    "    field_names = set()\n",
    "    for results in entity_results_list:\n",
    "        for result_json in results['results']:\n",
    "            row_json = _format_row_json(result_json)\n",
    "            field_names = field_names.union(row_json.keys())\n",
    "            row_jsons.append(row_json)\n",
    "\n",
    "    tsv_data = io.StringIO()\n",
    "    try:\n",
    "        field_name_list = sorted(list(field_names))\n",
    "        dict_writer = csv.DictWriter(tsv_data, field_name_list, dialect=csv.excel_tab)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(row_jsons)\n",
    "        table_df = pd.read_csv(io.StringIO(tsv_data.getvalue()), sep='\\t')\n",
    "\n",
    "        entity_id_column_name = f\"entity:{table_name}_id\"\n",
    "        if attributeNames is not None:\n",
    "            columns = sorted(attributeNames)\n",
    "        else:\n",
    "            columns = list(table_df.columns)\n",
    "            columns.remove(entity_id_column_name)\n",
    "        columns.insert(0, entity_id_column_name)\n",
    "        table_df = table_df[columns]\n",
    "    finally:\n",
    "        tsv_data.close()\n",
    "\n",
    "    return table_df\n",
    "\n",
    "def _format_row_json(result_json: dict) -> dict:\n",
    "    row_json = result_json['attributes']\n",
    "    for key, value in row_json.items():\n",
    "        # Process a reference\n",
    "        if type(value) == dict and 'entityType' in value:\n",
    "            assert key == value['entityType']\n",
    "            row_json[key] = value['entityName']\n",
    "    entity_id_name = f\"entity:{result_json['entityType']}_id\"\n",
    "    entity_id_value = result_json['name']\n",
    "    row_json[entity_id_name] = entity_id_value\n",
    "    return row_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _create_combined_merge_parameters(default_merge_parameters: dict, merge_info: dict) -> dict:\n",
    "    standard_pandas_default_parameters = dict(how=\"inner\", on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=(\"_x\", \"_y\"), copy=True, indicator=False, validate=None)\n",
    "    combined_parameters = standard_pandas_default_parameters.copy()\n",
    "    combined_parameters.update(default_merge_parameters)\n",
    "    if 'merge_parameters' in merge_info:\n",
    "        combined_parameters.update(merge_info['merge_parameters'])\n",
    "    if 'join_column' in merge_info:\n",
    "        combined_parameters['on'] = merge_info['join_column']\n",
    "    if 'join_type' in merge_info:\n",
    "        combined_parameters['how'] = merge_info['join_type']\n",
    "    return combined_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _substitute_entity_id_column_name(merge_parameters: dict) -> dict:\n",
    "    for key in 'on', 'left_on', 'right_on':\n",
    "        if key in merge_parameters and merge_parameters[key]:\n",
    "            merge_parameters[key] = _get_entity_id_column_name(merge_parameters[key])\n",
    "            # TODO - Add support for case where value is a list/array - requires careful testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _deduplicate_merge_data(merged_df: pd.DataFrame, current_df: pd.DataFrame,\n",
    "                           current_table_name: str, current_dedup_key: str) -> None:\n",
    "    # Some TOPMed projects (COPDGene, MESA, maybe others) are known to have multiple sample\n",
    "    # entries for the same subject. According to BioData Catalyst data experts,\n",
    "    # the duplicates should be equivalent, so just keep the first entry found in each case.\n",
    "\n",
    "    # Identify duplicates in the given column of the current table and obtain\n",
    "    # a list of entity ids for the rows containing duplicates.\n",
    "    # Then remove the duplicate rows from the current table.\n",
    "    current_dups = current_df[current_dedup_key].duplicated(keep=\"first\")\n",
    "    current_dups_values = current_df[current_dups][current_dedup_key].tolist()\n",
    "    if len(current_dups_values) == 0:\n",
    "        logger.debug(\"No duplicates found in table {} for key {}\".format(current_table_name, current_dedup_key))\n",
    "        return\n",
    "    current_table_entity_id = _get_entity_id_column_name(current_table_name)\n",
    "    common_key_values_for_dupes = current_df[current_dups][current_table_entity_id].tolist()\n",
    "    current_df.drop(current_df[current_dups].index, inplace=True)\n",
    "    logger.warning(\"Removed {} duplicate entries from table \\\"{}\\\" in column \\\"{}\\\". Retained the first entry found. Deleted rows with ids: {}\".format(\n",
    "        len(current_dups_values), current_table_name, current_dedup_key, current_dups_values))\n",
    "\n",
    "    # From the results that have been merged thus far, remove the rows that would have been joined\n",
    "    # to the rows that were deleted as duplicates from the current table. This will prevent \"orphan\"\n",
    "    # rows from being created in the consolidated dataframe, which would otherwise happen with\n",
    "    # some join types (e.g. \"outer\").\n",
    "    if merged_df is not None and current_table_entity_id in merged_df.columns:\n",
    "        mask = merged_df[current_table_entity_id].isin(common_key_values_for_dupes)\n",
    "        merged_df.drop(merged_df[mask].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _merge_dataframes(left_df: pd.DataFrame, right_df: pd.DataFrame, **merge_parameters) -> pd.DataFrame:\n",
    "    merged_df = left_df.merge(right_df, **merge_parameters)\n",
    "        \n",
    "    # Deduplicate \"*_entity_id\" columns\n",
    "    merged_df = merged_df.loc[:,~merged_df.columns.duplicated()]\n",
    "        \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _debug_write_df_to_tsv_file(df: pd.DataFrame, filename: str) -> None:\n",
    "    filename += \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S%f\") + \".tsv\"\n",
    "    write_df_to_tsv_file(df, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "@retry(reraise=True,\n",
    "       retry=retry_if_exception_type(FireCloudServerError), \n",
    "       stop=stop_after_attempt(5),\n",
    "       wait=wait_exponential(multiplier=4, min=10, max=60),\n",
    "       after=after_log(logger, logging.DEBUG),\n",
    "       before_sleep=before_sleep_log(logger, logging.INFO))\n",
    "def _fapi_list_entity_types(project: str, workspace: str):\n",
    "    response = fapi.list_entity_types(project, workspace)\n",
    "    fapi._check_response_code(response, 200)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "@retry(reraise=True,\n",
    "       retry=retry_if_exception_type(FireCloudServerError), \n",
    "       stop=stop_after_attempt(5),\n",
    "       wait=wait_exponential(multiplier=4, min=10, max=60),\n",
    "       after=after_log(logger, logging.DEBUG),\n",
    "       before_sleep=before_sleep_log(logger, logging.INFO))\n",
    "def _fapi_get_entities_tsv(project: str, workspace: str, table_name: str, attributeNames=None, model=\"flexible\"):\n",
    "    response = fapi.get_entities_tsv(project, workspace, table_name, attributeNames, model=model)\n",
    "    fapi._check_response_code(response, 200)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "@retry(reraise=True,\n",
    "       retry=retry_if_exception_type(FireCloudServerError),\n",
    "       stop=stop_after_attempt(5),\n",
    "       wait=wait_exponential(multiplier=4, min=10, max=60),\n",
    "       after=after_log(logger, logging.DEBUG),\n",
    "       before_sleep=before_sleep_log(logger, logging.INFO))\n",
    "def _fapi_get_entities_query(project: str, workspace: str, table_name: str,\n",
    "                             page=1, page_size=100, sort_direction=\"asc\", filter_terms=None):\n",
    "    response = fapi.get_entities_query(project, workspace, table_name, page, page_size, sort_direction, filter_terms)\n",
    "    fapi._check_response_code(response, 200)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6
    ],
    "pixiedust": {
     "displayParams": {}
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "@retry(reraise=True,\n",
    "       retry=retry_if_exception_type(FireCloudServerError), \n",
    "       stop=stop_after_attempt(5),\n",
    "       wait=wait_exponential(multiplier=4, min=10, max=60),\n",
    "       after=after_log(logger, logging.DEBUG),\n",
    "       before_sleep=before_sleep_log(logger, logging.INFO))\n",
    "def _fapi_upload_entities(project: str, workspace: str, entity_tsv: str, model: str):\n",
    "    response = fapi.upload_entities(project, workspace, entity_tsv, model)\n",
    "    fapi._check_response_code(response, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "@retry(reraise=True,\n",
    "       retry=retry_if_exception_type(FireCloudServerError), \n",
    "       stop=stop_after_attempt(3),\n",
    "       wait=wait_exponential(multiplier=4, min=5, max=30),\n",
    "       after=after_log(logger, logging.DEBUG),\n",
    "       before_sleep=before_sleep_log(logger, logging.INFO))\n",
    "def _fapi_update_entity(project: str, workspace: str, entity_type: str, entity_name: str, updates: list):\n",
    "    # See: https://software.broadinstitute.org/firecloud/documentation/article?id=10892\n",
    "    response = fapi.update_entity(project, workspace, entity_type, entity_name, updates)\n",
    "    fapi._check_response_code(response, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "@retry(reraise=True,\n",
    "       retry=retry_if_exception_type(FireCloudServerError), \n",
    "       stop=stop_after_attempt(3),\n",
    "       wait=wait_exponential(multiplier=4, min=5, max=30),\n",
    "       after=after_log(logger, logging.DEBUG),\n",
    "       before_sleep=before_sleep_log(logger, logging.INFO))\n",
    "def _fapi_delete_entity_type(namespace: str, workspace: str, etype: str, ename) -> dict:\n",
    "    response = fapi.delete_entity_type(namespace, workspace, etype, ename)\n",
    "    if response.status_code == 409:\n",
    "        message = f\"Please remove existing references to entities in {etype} and try again. References: {response.content}\"\n",
    "        logger.warning(message)\n",
    "        raise TerraDataUtilException(message)\n",
    "    fapi._check_response_code(response, 204)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _get_python_resource_usage() -> str:\n",
    "    usage = resource.getrusage(resource.RUSAGE_SELF)\n",
    "    memory_use_mb = math.ceil(usage.ru_maxrss / 1024)\n",
    "    display_string = f\"Python Memory Use: {memory_use_mb} mb\"\n",
    "    return display_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _estimate_total_duration(total_count: int, batch_size: int, batch_duration_seconds: float) -> str:\n",
    "    batches = math.ceil(total_count / batch_size)\n",
    "    total_duration_seconds = batches * batch_duration_seconds\n",
    "    return f\"{math.ceil(total_duration_seconds/60)} minutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _flush_log() -> None:\n",
    "    sys.stderr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _output_now(message: str) -> None:\n",
    "    sys.stderr.write(message)\n",
    "    sys.stderr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DataTableInfo:\n",
    "    \n",
    "    def __init__(self, project: str, workspace: str):\n",
    "        self.project = project\n",
    "        self.workspace = workspace\n",
    "        self._data_table_info = None\n",
    "        self._data_table_names = None\n",
    "\n",
    "    def refresh(self):\n",
    "        response = _fapi_list_entity_types(self.project, self.workspace)\n",
    "        assert response.status_code == 200\n",
    "        self._data_table_info = json.loads(response.text)\n",
    "        self._data_table_names = list(self._data_table_info.keys())\n",
    "\n",
    "    def get_data_table_info(self, refresh=False):\n",
    "        if not self._data_table_info or refresh:\n",
    "            self.refresh()\n",
    "        return self._data_table_info.copy()\n",
    "\n",
    "    def get_table_names(self, refresh=False):\n",
    "        if not self._data_table_names or refresh:\n",
    "            self.refresh()\n",
    "        return self._data_table_names.copy()\n",
    "\n",
    "    def get_table_info(self, table_name, refresh=False):\n",
    "        if not self._data_table_info or refresh:\n",
    "            self.refresh()\n",
    "        row_count = None\n",
    "        column_count = None\n",
    "        attributes = None\n",
    "        if table_name in self._data_table_names:\n",
    "            row_count = self._data_table_info[table_name]['count']\n",
    "            attributes = self._data_table_info[table_name]['attributeNames'].copy()\n",
    "            column_count = len(attributes) + 1  # Add one for the entity id column\n",
    "        return row_count, column_count, attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch the `FireCloudServerError` exception string to include the HTTP response code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _code_and_message(self):\n",
    "    return f\"code: {self.code} {self.message}\"\n",
    "FireCloudServerError.__str__ = _code_and_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TerraDataUtilException(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "        Exception.__init__(self, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Test/Debug Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the lines in the cells below to enable some built-in testing, improved debugging abilities or to serve as a simple stand-alone demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test with data in a different workspace than the one that contains this Notebook,\n",
    "specify remote workspace information below. This enables convenient testing of data\n",
    "for multiple different projects/cohorts using this same Notebook in the current workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['GOOGLE_PROJECT'] = os.environ['WORKSPACE_NAMESPACE'] = \"anvil-stage-demo\"\n",
    "# os.environ['WORKSPACE_NAME']=\"mbaumann terra_data_util test Amish\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set standard names used in this Notebook for these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BILLING_PROJECT_ID = os.environ['GOOGLE_PROJECT']\n",
    "# WORKSPACE = os.environ['WORKSPACE_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify, create and optionally delete the desired tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_example_consolidated_geno_pheno_table=True\n",
    "# create_example_consolidated_geno_table=True\n",
    "# create_example_consolidated_pheno_table=True\n",
    "# create_example_consolidated_custom_table=True\n",
    "# delete_created_tables=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if create_example_consolidated_geno_pheno_table:\n",
    "#     example_table_name = \"example_consolidated_geno_pheno_table\"\n",
    "#     consolidate_gen3_geno_pheno_tables(BILLING_PROJECT_ID, WORKSPACE, example_table_name)\n",
    "#     if delete_created_tables:\n",
    "#          delete_terra_table(BILLING_PROJECT_ID, WORKSPACE, example_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if create_example_consolidated_geno_table:\n",
    "#     example_table_name = \"example_consolidated_geno_table\"\n",
    "#     consolidate_gen3_geno_tables(BILLING_PROJECT_ID, WORKSPACE, example_table_name)\n",
    "#     if delete_created_tables:\n",
    "#          delete_terra_table(BILLING_PROJECT_ID, WORKSPACE, example_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if create_example_consolidated_pheno_table:\n",
    "#     example_table_name = \"example_consolidated_pheno_table\"\n",
    "#     consolidate_gen3_pheno_tables(BILLING_PROJECT_ID, WORKSPACE, example_table_name)\n",
    "#     if delete_created_tables:\n",
    "#          delete_terra_table(BILLING_PROJECT_ID, WORKSPACE, example_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if create_example_consolidated_custom_table:\n",
    "#     example_table_name = \"example_consolidated_custom_table\"\n",
    "#     consolidate_gen3_custom_tables(BILLING_PROJECT_ID, WORKSPACE, example_table_name)\n",
    "#     if delete_created_tables:\n",
    "#          delete_terra_table(BILLING_PROJECT_ID, WORKSPACE, example_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all tables\n",
    "# for example_table_name in \"example_consolidated_geno_pheno_table\", \"example_consolidated_geno_table\",\\\n",
    "# \"example_consolidated_pheno_table\", \"example_consolidated_custom_table\":\n",
    "#         try:\n",
    "#             logger.info(\"Deleting: {}\".format(example_table_name))\n",
    "#             delete_terra_table(BILLING_PROJECT_ID, WORKSPACE, example_table_name)\n",
    "#             logger.info(f\"Finished deleting:{}\".format(example_table_name))\n",
    "#         except Exception as ex:\n",
    "#             logger.warning(\"Table {} may not exist.\".format(example_table_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
